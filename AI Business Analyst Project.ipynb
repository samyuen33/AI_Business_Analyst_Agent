{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d8ef2d-ce54-4f49-f5e3-717f3bd8693a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries have been successfully installed!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# ðŸ“Œ Installing Required Libraries\n",
        "# ==================================================\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Function to install packages silently\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# List of required libraries\n",
        "libraries = [\n",
        "    \"langchain\",  # Core framework for working with LLMs\n",
        "    \"langchain-community\",  # Install the community package containing LLMs\n",
        "    \"openai==0.28\",  # OpenAI API package (version 0.28) for GPT models\n",
        "    \"langchain-huggingface\",  # Hugging Face LLM wrapper\n",
        "    \"google-generativeai\",  # For Google's Generative AI\n",
        "    \"langchain-google-genai\",  # LangChain integration with Google's Generative AI\n",
        "    \"colorama\"\n",
        "\n",
        "]\n",
        "\n",
        "# Install each library\n",
        "for library in libraries:\n",
        "    install(library)\n",
        "\n",
        "# Print a message when all libraries are done installing\n",
        "print(\"âœ… All libraries have been successfully installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd51db2-71ae-4c39-db25-369f20ad9b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All required libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# ðŸ“Œ Importing Required Libraries for LangChain Agent\n",
        "# ==================================================\n",
        "\n",
        "# âœ… System & Environment Setup\n",
        "import os  # For setting environment variables, such as API keys\n",
        "\n",
        "# âœ… Jupyter & Colab Utilities\n",
        "import ipywidgets as widgets  # For creating interactive input widgets\n",
        "from IPython.display import clear_output, display  # For managing notebook outputs\n",
        "\n",
        "# âœ… OpenAI API\n",
        "import openai  # Direct interaction with OpenAI API (useful for API-based calls)\n",
        "\n",
        "# Gemini API\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# âœ… LangChain Components\n",
        "from langchain.llms import OpenAI  # Wrapper for interacting with OpenAI LLMs\n",
        "from langchain.chat_models import ChatOpenAI  # For chat-based OpenAI models\n",
        "from langchain.agents import AgentType, initialize_agent  # For creating AI agents\n",
        "from langchain.tools import Tool  # For adding external tools to agents\n",
        "from langchain.memory import ConversationBufferMemory  # For maintaining conversation history\n",
        "from langchain.prompts import PromptTemplate  # For creating structured prompts\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Hugging Face\n",
        "# Hugging Face\n",
        "from langchain.llms import HuggingFaceHub  # Import HuggingFaceHub from langchain.llms\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# âœ… Hugging Face Transformers (Only required if using Hugging Face models)\n",
        "import transformers  # Hugging Face library for pre-trained models\n",
        "\n",
        "# âœ… Confirmation message\n",
        "print(\"âœ… All required libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enter API Keys and upload CSV file below\n"
      ],
      "metadata": {
        "id": "wyHmL8D_46Ec"
      },
      "id": "wyHmL8D_46Ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637,
          "referenced_widgets": [
            "da110c8e688243e1ab6a64fa8ea388c9",
            "dd741e70b1cc43d39f52e3913f6ae972",
            "7dbcbd1767c24237ad3f22d611c17909",
            "5bb5774044e5471399a1efcded104709",
            "1ec7c323c6b540eb99d8a8ebf73f1b80",
            "449547ebe3ec48e1bc746450d8a34ee4",
            "ac400e4cb9884ed69e236861246cfc68",
            "e4dfb0fd85d74187a58128502fdb178b",
            "4fdf2ccf7f414527a1d89319118c94e6",
            "327e0dd4465748498e7e93bf46e15055",
            "ff9b522561fd4abe855ccfe3ab595cad",
            "db75bb5ea3c44398b69e38107d1ceb8f",
            "18ebc14e9a0a446ea2ddb01f61137c6f",
            "03c858a64d3d4f7cbcd29b75cbd9bfcd",
            "c7e4131c636f4cac8510e7d3bd5fa280",
            "991cd48c490d48aca70f64e850e4ec60",
            "df867a363cb14fe08d0eba405a644f1c",
            "d1b574100af04142b9d68ec584434d5a"
          ]
        },
        "outputId": "7551a7bb-6475-4a41-b7ec-290ce804dcf4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='ðŸ”‘ OpenAI Key:', placeholder='Enter your OpenAI API Key')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da110c8e688243e1ab6a64fa8ea388c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='ðŸ¤— HF Key:', placeholder='Enter your Hugging Face API Key')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bb5774044e5471399a1efcded104709"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='âœ¨ Gemini Key:', placeholder='Enter your Gemini API Key')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac400e4cb9884ed69e236861246cfc68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='âœ… Set API Keys', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "327e0dd4465748498e7e93bf46e15055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenAI API Key has been set successfully!\n",
            "âœ… Hugging Face API Key has been set successfully!\n",
            "âœ… Gemini API Key has been set successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18ebc14e9a0a446ea2ddb01f61137c6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='âœ… Process CSV', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "991cd48c490d48aca70f64e850e4ec60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0     Retailer  Retailer ID Invoice Date     Region     State  \\\n",
              "0         NaN  Foot Locker      1185732   01/01/2020  Northeast  New York   \n",
              "1         NaN  Foot Locker      1185732   01/02/2020  Northeast  New York   \n",
              "2         NaN  Foot Locker      1185732   01/03/2020  Northeast  New York   \n",
              "3         NaN  Foot Locker      1185732   01/04/2020  Northeast  New York   \n",
              "4         NaN  Foot Locker      1185732   01/05/2020  Northeast  New York   \n",
              "\n",
              "       City                    Product  Price per Unit  Units Sold  \\\n",
              "0  New York      Men's Street Footwear              50        1200   \n",
              "1  New York    Men's Athletic Footwear              50        1000   \n",
              "2  New York    Women's Street Footwear              40        1000   \n",
              "3  New York  Women's Athletic Footwear              45         850   \n",
              "4  New York              Men's Apparel              60         900   \n",
              "\n",
              "   Total Sales  Operating Profit Operating Margin Sales Method  \n",
              "0     600000.0          300000.0              50%     In-store  \n",
              "1     500000.0          150000.0              30%     In-store  \n",
              "2     400000.0          140000.0              35%     In-store  \n",
              "3     382500.0          133875.0              35%     In-store  \n",
              "4     540000.0          162000.0              30%     In-store  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49ce11f1-fff2-4316-9b2c-8375433e3f72\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Retailer</th>\n",
              "      <th>Retailer ID</th>\n",
              "      <th>Invoice Date</th>\n",
              "      <th>Region</th>\n",
              "      <th>State</th>\n",
              "      <th>City</th>\n",
              "      <th>Product</th>\n",
              "      <th>Price per Unit</th>\n",
              "      <th>Units Sold</th>\n",
              "      <th>Total Sales</th>\n",
              "      <th>Operating Profit</th>\n",
              "      <th>Operating Margin</th>\n",
              "      <th>Sales Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Foot Locker</td>\n",
              "      <td>1185732</td>\n",
              "      <td>01/01/2020</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>New York</td>\n",
              "      <td>New York</td>\n",
              "      <td>Men's Street Footwear</td>\n",
              "      <td>50</td>\n",
              "      <td>1200</td>\n",
              "      <td>600000.0</td>\n",
              "      <td>300000.0</td>\n",
              "      <td>50%</td>\n",
              "      <td>In-store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Foot Locker</td>\n",
              "      <td>1185732</td>\n",
              "      <td>01/02/2020</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>New York</td>\n",
              "      <td>New York</td>\n",
              "      <td>Men's Athletic Footwear</td>\n",
              "      <td>50</td>\n",
              "      <td>1000</td>\n",
              "      <td>500000.0</td>\n",
              "      <td>150000.0</td>\n",
              "      <td>30%</td>\n",
              "      <td>In-store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Foot Locker</td>\n",
              "      <td>1185732</td>\n",
              "      <td>01/03/2020</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>New York</td>\n",
              "      <td>New York</td>\n",
              "      <td>Women's Street Footwear</td>\n",
              "      <td>40</td>\n",
              "      <td>1000</td>\n",
              "      <td>400000.0</td>\n",
              "      <td>140000.0</td>\n",
              "      <td>35%</td>\n",
              "      <td>In-store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Foot Locker</td>\n",
              "      <td>1185732</td>\n",
              "      <td>01/04/2020</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>New York</td>\n",
              "      <td>New York</td>\n",
              "      <td>Women's Athletic Footwear</td>\n",
              "      <td>45</td>\n",
              "      <td>850</td>\n",
              "      <td>382500.0</td>\n",
              "      <td>133875.0</td>\n",
              "      <td>35%</td>\n",
              "      <td>In-store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Foot Locker</td>\n",
              "      <td>1185732</td>\n",
              "      <td>01/05/2020</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>New York</td>\n",
              "      <td>New York</td>\n",
              "      <td>Men's Apparel</td>\n",
              "      <td>60</td>\n",
              "      <td>900</td>\n",
              "      <td>540000.0</td>\n",
              "      <td>162000.0</td>\n",
              "      <td>30%</td>\n",
              "      <td>In-store</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49ce11f1-fff2-4316-9b2c-8375433e3f72')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49ce11f1-fff2-4316-9b2c-8375433e3f72 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49ce11f1-fff2-4316-9b2c-8375433e3f72');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3cf3a333-aa2d-4f73-a69e-3044f2adebd9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3cf3a333-aa2d-4f73-a69e-3044f2adebd9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3cf3a333-aa2d-4f73-a69e-3044f2adebd9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(openai_key_input, huggingface_key_input, gemini_key_input, submit_button)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Retailer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Retailer ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1185732,\n        \"max\": 1185732,\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Invoice Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"State\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price per Unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 40,\n        \"max\": 60,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Units Sold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 134,\n        \"min\": 850,\n        \"max\": 1200,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 92472.96902338542,\n        \"min\": 382500.0,\n        \"max\": 600000.0,\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Operating Profit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 69483.11395583821,\n        \"min\": 133875.0,\n        \"max\": 300000.0,\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Operating Margin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sales Method\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV has been uploaded and processed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Define global variables\n",
        "hf_key = None\n",
        "gemini_key = None\n",
        "openai_key = None\n",
        "\n",
        "# âœ… Create input widgets for API keys\n",
        "openai_key_input = widgets.Password(\n",
        "    description=\"ðŸ”‘ OpenAI Key:\",\n",
        "    placeholder=\"Enter your OpenAI API Key\",\n",
        ")\n",
        "\n",
        "huggingface_key_input = widgets.Password(\n",
        "    description=\"ðŸ¤— HF Key:\",\n",
        "    placeholder=\"Enter your Hugging Face API Key\",\n",
        ")\n",
        "\n",
        "gemini_key_input = widgets.Password(\n",
        "    description=\"âœ¨ Gemini Key:\",\n",
        "    placeholder=\"Enter your Gemini API Key\",\n",
        ")\n",
        "\n",
        "# âœ… Create a button to submit API keys\n",
        "submit_button = widgets.Button(description=\"âœ… Set API Keys\")\n",
        "\n",
        "# âœ… Function to save API keys when the button is clicked\n",
        "def set_api_keys(b):\n",
        "    global hf_key, gemini_key, openai_key\n",
        "\n",
        "    # Retrieve and validate API keys\n",
        "    openai_key = openai_key_input.value.strip()\n",
        "    hf_key = huggingface_key_input.value.strip()\n",
        "    gemini_key = gemini_key_input.value.strip()\n",
        "\n",
        "    # âœ… Set OpenAI API Key\n",
        "    if openai_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "        openai.api_key = openai_key\n",
        "        print(\"âœ… OpenAI API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"âŒ Please enter a valid OpenAI API Key.\")\n",
        "\n",
        "    # âœ… Set Hugging Face API Key\n",
        "    if hf_key:\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "        print(\"âœ… Hugging Face API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"âŒ Please enter a valid Hugging Face API Key.\")\n",
        "\n",
        "    # âœ… Set Gemini API Key\n",
        "    if gemini_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = gemini_key\n",
        "        genai.configure(api_key=gemini_key)\n",
        "        print(\"âœ… Gemini API Key has been set successfully!\")\n",
        "    else:\n",
        "        print(\"âŒ Please enter a valid Gemini API Key.\")\n",
        "\n",
        "    # Now display file upload widgets after API keys are set\n",
        "    upload_button = widgets.FileUpload(accept=\".csv\", multiple=False)\n",
        "    process_button = widgets.Button(description=\"âœ… Process CSV\")\n",
        "\n",
        "    # Function to handle file upload\n",
        "    def handle_file_upload(b):\n",
        "        uploaded_file = list(upload_button.value.values())[0]\n",
        "        content = uploaded_file['content']\n",
        "        import builtins\n",
        "        builtins.df = pd.read_csv(io.BytesIO(content))\n",
        "        display(builtins.df.head())  # Display first few rows of the CSV\n",
        "        print(\"CSV has been uploaded and processed!\")\n",
        "\n",
        "\n",
        "    # Link the file upload button to the handler function\n",
        "    process_button.on_click(handle_file_upload)\n",
        "\n",
        "    # Display the upload button and process button\n",
        "    display(upload_button, process_button)\n",
        "\n",
        "# âœ… Link button click to the function\n",
        "submit_button.on_click(set_api_keys)\n",
        "\n",
        "# âœ… Display the input fields and button\n",
        "display(openai_key_input, huggingface_key_input, gemini_key_input, submit_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run everything below. The last 3 cells are for choosing which model to run."
      ],
      "metadata": {
        "id": "zufAjcZX5XLp"
      },
      "id": "zufAjcZX5XLp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3 8b Agent"
      ],
      "metadata": {
        "id": "C2XF3RZW1F2I"
      },
      "id": "C2XF3RZW1F2I"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from typing import Optional\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from IPython.display import Image, display\n",
        "import base64\n",
        "import re\n",
        "import builtins\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# 1. Data Metrics Calculator Tool\n",
        "def calculate_metrics(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate statistical metrics from the dataframe based on specified columns.\n",
        "    Input should be a string containing column names and desired metrics.\n",
        "    Example: 'Calculate mean, median, and standard deviation for sales_revenue'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Parse input to identify columns and metrics\n",
        "        metrics_map = {\n",
        "            'mean': 'mean',\n",
        "            'average': 'mean',\n",
        "            'median': 'median',\n",
        "            'min': 'min',\n",
        "            'minimum': 'min',\n",
        "            'max': 'max',\n",
        "            'maximum': 'max',\n",
        "            'std': 'std',\n",
        "            'standard deviation': 'std',\n",
        "            'sum': 'sum',\n",
        "            'count': 'count',\n",
        "            'unique': 'nunique',\n",
        "            'correlation': 'corr'\n",
        "        }\n",
        "\n",
        "        # Check for correlation request (special case)\n",
        "        if 'correlation' in input_string.lower() or 'corr' in input_string.lower():\n",
        "            if 'between' in input_string.lower() and 'and' in input_string.lower():\n",
        "                parts = input_string.lower().split('between')[1].split('and')\n",
        "                col1 = parts[0].strip()\n",
        "                col2 = parts[1].strip().split()[0].strip()\n",
        "\n",
        "                # Check if columns exist\n",
        "                if col1 in df.columns and col2 in df.columns:\n",
        "                    corr_value = df[col1].corr(df[col2])\n",
        "                    return f\"The correlation between {col1} and {col2} is {corr_value:.4f}\"\n",
        "                else:\n",
        "                    return f\"Columns not found. Available columns are: {', '.join(df.columns)}\"\n",
        "            else:\n",
        "                # Return full correlation matrix\n",
        "                corr_matrix = df.select_dtypes(include=['number']).corr()\n",
        "                return f\"Correlation Matrix:\\n{corr_matrix.round(2).to_string()}\"\n",
        "\n",
        "        # For other metrics\n",
        "        requested_metrics = []\n",
        "        for metric_name, func_name in metrics_map.items():\n",
        "            if metric_name in input_string.lower():\n",
        "                requested_metrics.append(func_name)\n",
        "\n",
        "        # If no specific metrics mentioned, calculate basic stats\n",
        "        if not requested_metrics:\n",
        "            requested_metrics = ['mean', 'median', 'min', 'max', 'std']\n",
        "\n",
        "        # Find requested columns\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # If no specific columns mentioned, use all numeric columns\n",
        "        if not columns:\n",
        "            columns = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "        # Calculate metrics\n",
        "        results = {}\n",
        "        for col in columns:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            col_results = {}\n",
        "            for metric in requested_metrics:\n",
        "                if metric == 'mean':\n",
        "                    col_results['mean'] = df[col].mean()\n",
        "                elif metric == 'median':\n",
        "                    col_results['median'] = df[col].median()\n",
        "                elif metric == 'min':\n",
        "                    col_results['min'] = df[col].min()\n",
        "                elif metric == 'max':\n",
        "                    col_results['max'] = df[col].max()\n",
        "                elif metric == 'std':\n",
        "                    col_results['std'] = df[col].std()\n",
        "                elif metric == 'sum':\n",
        "                    col_results['sum'] = df[col].sum()\n",
        "                elif metric == 'count':\n",
        "                    col_results['count'] = df[col].count()\n",
        "                elif metric == 'nunique':\n",
        "                    col_results['unique_values'] = df[col].nunique()\n",
        "\n",
        "            results[col] = col_results\n",
        "\n",
        "        # Format results\n",
        "        output = \"Data Metrics Analysis:\\n\"\n",
        "        for col, metrics in results.items():\n",
        "            output += f\"\\n{col}:\\n\"\n",
        "            for metric_name, value in metrics.items():\n",
        "                # Use format specifier only if value is a float\n",
        "                if isinstance(value, float):\n",
        "                    output += f\"  - {metric_name}: {value:.4f}\\n\"\n",
        "                else:\n",
        "                    output += f\"  - {metric_name}: {value}\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating metrics: {str(e)}. Please check column names and metrics requested.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# 2. Chart Generation Tool\n",
        "def generate_chart(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate charts based on columns and chart type and save to local filesystem.\n",
        "    Input should specify columns to visualize and optionally chart type.\n",
        "    Example: 'Create a chart of sales vs time' or 'Plot revenue against month as bar chart'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify columns mentioned\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # Identify chart type\n",
        "        chart_type = None\n",
        "        if 'bar' in input_string.lower() or 'histogram' in input_string.lower():\n",
        "            chart_type = 'bar'\n",
        "        elif 'scatter' in input_string.lower():\n",
        "            chart_type = 'scatter'\n",
        "        elif 'line' in input_string.lower():\n",
        "            chart_type = 'line'\n",
        "        elif 'pie' in input_string.lower():\n",
        "            chart_type = 'pie'\n",
        "        elif 'box' in input_string.lower() or 'boxplot' in input_string.lower():\n",
        "            chart_type = 'box'\n",
        "        elif 'heat' in input_string.lower() or 'heatmap' in input_string.lower():\n",
        "            chart_type = 'heatmap'\n",
        "\n",
        "        # If no columns specified or found\n",
        "        if not columns:\n",
        "            return f\"Please specify which columns to chart. Available columns: {', '.join(df.columns)}\"\n",
        "\n",
        "        # Determine best chart type if not specified\n",
        "        if not chart_type:\n",
        "            if len(columns) == 1:\n",
        "                # For a single column, use histogram for numeric, bar chart for categorical\n",
        "                if df[columns[0]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'histogram'\n",
        "                else:\n",
        "                    chart_type = 'bar'\n",
        "            elif len(columns) == 2:\n",
        "                # For two columns, if both numeric use scatter plot\n",
        "                if df[columns[0]].dtype in ['int64', 'float64'] and df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'scatter'\n",
        "                # If one is categorical and other numeric, use bar or line\n",
        "                elif df[columns[0]].dtype in ['int64', 'float64'] or df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    if 'time' in columns[0].lower() or 'date' in columns[0].lower() or 'month' in columns[0].lower() or 'year' in columns[0].lower():\n",
        "                        chart_type = 'line'\n",
        "                    else:\n",
        "                        chart_type = 'bar'\n",
        "                else:\n",
        "                    chart_type = 'heatmap'\n",
        "            else:\n",
        "                # For more than two columns, default to correlation heatmap for numeric columns\n",
        "                chart_type = 'heatmap'\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        if chart_type == 'bar':\n",
        "            if len(columns) == 1:\n",
        "                sns.countplot(x=columns[0], data=df)\n",
        "                plt.title(f\"Count of {columns[0]}\")\n",
        "            else:\n",
        "                # Assuming first column is x-axis, second is y-axis\n",
        "                sns.barplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"{columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'histogram':\n",
        "            sns.histplot(df[columns[0]], kde=True)\n",
        "            plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'scatter':\n",
        "            sns.scatterplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} vs {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'line':\n",
        "            sns.lineplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} over {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'pie':\n",
        "            # For pie charts, limit to top categories if categorical\n",
        "            if df[columns[0]].nunique() > 10:\n",
        "                top_values = df[columns[0]].value_counts().nlargest(10)\n",
        "                plt.pie(top_values, labels=top_values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Top 10 Categories in {columns[0]}\")\n",
        "            else:\n",
        "                values = df[columns[0]].value_counts()\n",
        "                plt.pie(values, labels=values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'box':\n",
        "            if len(columns) == 1:\n",
        "                sns.boxplot(y=columns[0], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[0]}\")\n",
        "            else:\n",
        "                sns.boxplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'heatmap':\n",
        "            if len(columns) == 2:\n",
        "                # Create a crosstab for categorical columns\n",
        "                cross_tab = pd.crosstab(df[columns[0]], df[columns[1]])\n",
        "                sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\", fmt='d')\n",
        "                plt.title(f\"Heatmap of {columns[0]} vs {columns[1]}\")\n",
        "            else:\n",
        "                # Correlation heatmap for multiple numeric columns\n",
        "                corr_columns = [col for col in columns if df[col].dtype in ['int64', 'float64']]\n",
        "                if not corr_columns:\n",
        "                    return \"Cannot create correlation heatmap - no numeric columns specified.\"\n",
        "                corr_matrix = df[corr_columns].corr()\n",
        "                sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "                plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Generate a descriptive filename based on chart type and columns\n",
        "        import os\n",
        "        import datetime\n",
        "\n",
        "        # Create charts directory if it doesn't exist\n",
        "        charts_dir = \"data_charts\"\n",
        "        if not os.path.exists(charts_dir):\n",
        "            os.makedirs(charts_dir)\n",
        "\n",
        "        # Create filename with timestamp and chart details\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        columns_str = \"_\".join([col.replace(\" \", \"_\") for col in columns])\n",
        "        filename = f\"{charts_dir}/{chart_type}_{columns_str}_{timestamp}.png\"\n",
        "\n",
        "        # Save the chart to the file\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Return information about the saved file\n",
        "        chart_info = f\"Generated a {chart_type} chart using columns: {', '.join(columns)}. \\nChart saved to: {os.path.abspath(filename)}\"\n",
        "\n",
        "        return chart_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating chart: {str(e)}. Please check your column names and chart request.\"\n",
        "\n",
        "# 3. Trend Analysis Tool\n",
        "def analyze_trends(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze trends in the data based on specified columns.\n",
        "    Input should specify which columns to analyze for trends.\n",
        "    Example: 'Analyze trends in monthly sales revenue' or 'Find patterns in customer acquisition by region'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify time-related columns\n",
        "        time_columns = []\n",
        "        for col in df.columns:\n",
        "            if any(term in col.lower() for term in ['time', 'date', 'month', 'year', 'quarter', 'week', 'day']):\n",
        "                time_columns.append(col)\n",
        "\n",
        "        # Identify metric columns mentioned in the input\n",
        "        metric_columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower() and col not in time_columns:\n",
        "                metric_columns.append(col)\n",
        "\n",
        "        # If no specific metric columns mentioned, identify numeric columns that might be metrics\n",
        "        if not metric_columns:\n",
        "            potential_metrics = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            # Filter out any time-related columns from potential metrics\n",
        "            metric_columns = [col for col in potential_metrics if col not in time_columns]\n",
        "\n",
        "        # If no time column found, try to find one\n",
        "        if not time_columns:\n",
        "            # Look for columns that might represent sequences or order\n",
        "            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            for col in numeric_cols:\n",
        "                # Check if column has sequential values that might represent time\n",
        "                if df[col].nunique() > 5 and df[col].is_monotonic_increasing:\n",
        "                    time_columns.append(col)\n",
        "                    break\n",
        "\n",
        "            # If still no time column, check if index might represent time\n",
        "            if not time_columns and df.index.is_monotonic_increasing:\n",
        "                df['index_as_sequence'] = df.index\n",
        "                time_columns.append('index_as_sequence')\n",
        "\n",
        "        # If we have both time and metric columns, analyze trends\n",
        "        results = []\n",
        "        if time_columns and metric_columns:\n",
        "            primary_time_col = time_columns[0]\n",
        "\n",
        "            for metric in metric_columns:\n",
        "                if df[metric].dtype not in ['int64', 'float64']:\n",
        "                    continue\n",
        "\n",
        "                # Basic trend statistics\n",
        "                trend_data = df[[primary_time_col, metric]].dropna()\n",
        "\n",
        "                if len(trend_data) < 3:  # Need at least 3 points for meaningful trend\n",
        "                    results.append(f\"Insufficient data points for {metric} trend analysis\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate basic trend metrics\n",
        "                first_value = trend_data[metric].iloc[0]\n",
        "                last_value = trend_data[metric].iloc[-1]\n",
        "                overall_change = last_value - first_value\n",
        "                percent_change = (overall_change / first_value * 100) if first_value != 0 else float('inf')\n",
        "\n",
        "                # Calculate moving averages if enough data points\n",
        "                if len(trend_data) >= 5:\n",
        "                    trend_data['3pt_ma'] = trend_data[metric].rolling(window=3).mean()\n",
        "\n",
        "                    # Detect direction (increasing/decreasing/fluctuating)\n",
        "                    increases = 0\n",
        "                    decreases = 0\n",
        "                    for i in range(1, len(trend_data)):\n",
        "                        if trend_data[metric].iloc[i] > trend_data[metric].iloc[i-1]:\n",
        "                            increases += 1\n",
        "                        elif trend_data[metric].iloc[i] < trend_data[metric].iloc[i-1]:\n",
        "                            decreases += 1\n",
        "\n",
        "                    # Determine trend direction\n",
        "                    if increases > decreases * 2:\n",
        "                        direction = \"strongly increasing\"\n",
        "                    elif increases > decreases:\n",
        "                        direction = \"moderately increasing\"\n",
        "                    elif decreases > increases * 2:\n",
        "                        direction = \"strongly decreasing\"\n",
        "                    elif decreases > increases:\n",
        "                        direction = \"moderately decreasing\"\n",
        "                    else:\n",
        "                        direction = \"fluctuating without clear direction\"\n",
        "\n",
        "                    # Check for seasonality (simplistic approach)\n",
        "                    if len(trend_data) >= 12:\n",
        "                        diffs = []\n",
        "                        for i in range(1, len(trend_data)):\n",
        "                            diffs.append(trend_data[metric].iloc[i] - trend_data[metric].iloc[i-1])\n",
        "                        sign_changes = sum(1 for i in range(1, len(diffs)) if (diffs[i] > 0 and diffs[i-1] < 0) or (diffs[i] < 0 and diffs[i-1] > 0))\n",
        "                        if sign_changes >= len(diffs) // 3:\n",
        "                            seasonality = \"data shows possible cyclical patterns\"\n",
        "                        else:\n",
        "                            seasonality = \"no clear seasonal patterns detected\"\n",
        "                    else:\n",
        "                        seasonality = \"insufficient data to detect seasonality\"\n",
        "\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Direction: {direction}\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Volatility: {trend_data[metric].std():.2f}\\n\"\n",
        "                    result += f\"- {seasonality}\\n\"\n",
        "                else:\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Limited data points available for detailed trend analysis\\n\"\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "            # Combine results\n",
        "            final_output = \"\\n\".join(results)\n",
        "            return final_output\n",
        "        else:\n",
        "            if not time_columns:\n",
        "                return \"Could not identify any time-related columns for trend analysis. Please specify which column represents time or sequence.\"\n",
        "            if not metric_columns:\n",
        "                return f\"Please specify which metrics to analyze for trends. Available numeric columns: {', '.join(df.select_dtypes(include=['number']).columns)}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing trends: {str(e)}. Please check your column names and trend request.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# 4. Data Summary Tool\n",
        "def summarize_data_contents(input_string: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Summarize the contents of the dataframe, including column names, data types,\n",
        "    and sample values for each column to help understand the data structure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Get basic dataframe info\n",
        "        num_rows, num_cols = df.shape\n",
        "        column_info = []\n",
        "\n",
        "        # Build summary for each column\n",
        "        for column in df.columns:\n",
        "            col_type = str(df[column].dtype)\n",
        "            unique_values = df[column].nunique()\n",
        "\n",
        "            # Get sample values based on data type and uniqueness\n",
        "            if unique_values <= 10:\n",
        "                # For columns with few unique values, show all of them with counts\n",
        "                value_counts = df[column].value_counts().head(10).to_dict()\n",
        "                sample_values = \", \".join([f\"{k} ({v})\" for k, v in value_counts.items()])\n",
        "            elif df[column].dtype == 'object':\n",
        "                # For text columns, show a few samples\n",
        "                sample_values = \", \".join([f'\"{x}\"' for x in df[column].dropna().sample(min(5, len(df))).tolist()])\n",
        "            else:\n",
        "                # For numeric columns, show range and a few examples\n",
        "                sample_values = f\"Range: {df[column].min()} to {df[column].max()}, \"\n",
        "                sample_values += f\"Examples: {', '.join(map(str, df[column].dropna().sample(min(3, len(df))).tolist()))}\"\n",
        "\n",
        "            # Check for missing values\n",
        "            missing = df[column].isna().sum()\n",
        "            missing_pct = missing / len(df) * 100\n",
        "\n",
        "            column_info.append({\n",
        "                \"column\": column,\n",
        "                \"type\": col_type,\n",
        "                \"unique_values\": unique_values,\n",
        "                \"missing\": f\"{missing} ({missing_pct:.1f}%)\",\n",
        "                \"sample_values\": sample_values\n",
        "            })\n",
        "\n",
        "        # Format the output\n",
        "        result = f\"DataFrame Summary: {num_rows} rows Ã— {num_cols} columns\\n\\n\"\n",
        "\n",
        "        for info in column_info:\n",
        "            result += f\"Column: {info['column']}\\n\"\n",
        "            result += f\"  - Type: {info['type']}\\n\"\n",
        "            result += f\"  - Unique Values: {info['unique_values']}\\n\"\n",
        "            result += f\"  - Missing Values: {info['missing']}\\n\"\n",
        "            result += f\"  - Sample Values: {info['sample_values']}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error summarizing data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# 5. Data Availability Tool\n",
        "def check_data_availability(input_string: str = \"\") -> str:\n",
        "    try:\n",
        "        if hasattr(builtins, 'df') and builtins.df is not None:\n",
        "            columns = list(builtins.df.columns)\n",
        "            return f\"Data is available with {len(builtins.df)} rows and {len(columns)} columns: {', '.join(columns)}\"\n",
        "        else:\n",
        "            return \"No data has been loaded yet. Please upload a CSV file first.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error checking data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# Create Tools\n",
        "metric_calculator = Tool(\n",
        "    name=\"calculate_metrics\",\n",
        "    func=calculate_metrics,\n",
        "    description=\"Calculate statistical metrics from the dataframe. Use when asked about averages, totals, statistics, correlations, or any numerical analysis of the data.\"\n",
        ")\n",
        "\n",
        "chart_generator = Tool(\n",
        "    name=\"generate_chart\",\n",
        "    func=generate_chart,\n",
        "    description=\"Generate charts and visualizations from the dataframe. Use when asked to plot, chart, visualize, or graph any data columns.\"\n",
        ")\n",
        "\n",
        "trend_analyzer = Tool(\n",
        "    name=\"analyze_trends\",\n",
        "    func=analyze_trends,\n",
        "    description=\"Analyze trends, patterns, and changes over time in the data. Use when asked about growth, decline, seasonality, or patterns in the data.\"\n",
        ")\n",
        "\n",
        "data_summarizer = Tool(\n",
        "    name=\"summarize_data\",\n",
        "    func=summarize_data_contents,\n",
        "    description=\"Analyze and summarize the contents of each column in the dataframe, including data types, unique values, and sample values. Use when asked about the structure of the data, what values are in columns, or to understand the dataset contents.\"\n",
        ")\n",
        "\n",
        "\n",
        "data_checker = Tool(\n",
        "    name=\"check_data\",\n",
        "    func=check_data_availability,\n",
        "    description=\"Check if data is loaded and show basic information about the available dataframe.\"\n",
        ")\n",
        "\n",
        "# Create the conversational prompt template\n",
        "conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a knowledgeable and insightful business analyst assistant.\n",
        "    Your goal is to help users analyze and understand their business data through clear explanations and visualizations.\n",
        "\n",
        "    When interacting with the user:\n",
        "    1. Help them understand what insights they can extract from their data\n",
        "    2. Explain business metrics and trends in plain language\n",
        "    3. Suggest appropriate visualizations based on the data type. If you do create a chart, tell them where to find it.\n",
        "    4. Provide context for statistical findings\n",
        "    5. Suggest follow-up analyses when appropriate\n",
        "\n",
        "    Always verify that data is available before attempting analysis.\n",
        "    If the user hasn't uploaded data yet, kindly remind them to upload a CSV file first.\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "\n",
        "# Create the inference client\n",
        "inference_client = InferenceClient(\n",
        "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    token=hf_key\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",  # Specify repo_id here\n",
        "    huggingfacehub_api_token=hf_key,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=2048,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Create the conversational chain\n",
        "conversation_chain = conversation_prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# Custom process function to handle agent output with embedded images\n",
        "def process_agent_output(output):\n",
        "    # Check if output contains base64 image data\n",
        "    if \"data:image/png;base64,\" in output:\n",
        "        # Extract info text (part before the base64 data)\n",
        "        info_text = output.split(\"data:image/png;base64,\")[0].strip()\n",
        "\n",
        "        # Extract base64 data\n",
        "        base64_pattern = r\"data:image/png;base64,([^\\\"'\\s]+)\"\n",
        "        match = re.search(base64_pattern, output)\n",
        "\n",
        "        if match:\n",
        "            base64_data = match.group(1)\n",
        "            try:\n",
        "                # Decode base64 and display image\n",
        "                image_data = base64.b64decode(base64_data)\n",
        "                display(Image(data=image_data))\n",
        "\n",
        "                # Return only the text part of the response\n",
        "                return info_text\n",
        "            except Exception as e:\n",
        "                return f\"{info_text}\\n[Error displaying visualization: {str(e)}]\"\n",
        "        else:\n",
        "            return output\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "# Modified initialize_agent call with a custom callback to process the output\n",
        "class AgentOutputHandler:\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def run(self, input_text):\n",
        "        response = self.agent.run(input_text)\n",
        "        return process_agent_output(response)\n",
        "\n",
        "# Initialize agent with tools\n",
        "conversation_agent = initialize_agent(\n",
        "    tools=[data_checker, metric_calculator, chart_generator, trend_analyzer, data_summarizer],\n",
        "    llm=model,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"output\"\n",
        "    ),\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=10\n",
        ")\n",
        "\n",
        "# Function to start the business analysis session\n",
        "import textwrap\n",
        "from colorama import Fore, Style\n",
        "\n",
        "# Function to start the business analysis session\n",
        "def start_llama_session():\n",
        "    print(f\"{Fore.GREEN}Business Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?{Style.RESET_ALL}\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")  # Regular input prompt\n",
        "        print(f\"{Fore.BLUE}You: {user_input}{Style.RESET_ALL}\")  # Display user input in blue\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye', \"fuck you\", \"fuck off\"]:\n",
        "            print(f\"{Fore.GREEN}Business Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.{Style.RESET_ALL}\")\n",
        "            break\n",
        "\n",
        "        # Direct agent response - no more processing needed\n",
        "        response = conversation_agent.run(user_input)\n",
        "\n",
        "        # Wrap the response text to a fixed width for better readability\n",
        "        wrapped_response = textwrap.fill(response, width=80)  # Adjust width as needed\n",
        "        print(f\"{Fore.GREEN}Business Analyst Assistant:\\n{wrapped_response}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "kDA2u6h5Hvlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8818deeb-3883-4e9d-9465-5da142eb4116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d9e3a222a688>:612: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationBufferMemory(\n",
            "<ipython-input-4-d9e3a222a688>:607: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  conversation_agent = initialize_agent(\n"
          ]
        }
      ],
      "id": "kDA2u6h5Hvlu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 4o mini Agent"
      ],
      "metadata": {
        "id": "6YdLi_UA1Wf5"
      },
      "id": "6YdLi_UA1Wf5"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from typing import Optional\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from IPython.display import Image, display\n",
        "import base64\n",
        "import re\n",
        "import builtins\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# 1. Data Metrics Calculator Tool\n",
        "def calculate_metrics(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate statistical metrics from the dataframe based on specified columns.\n",
        "    Input should be a string containing column names and desired metrics.\n",
        "    Example: 'Calculate mean, median, and standard deviation for sales_revenue'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Parse input to identify columns and metrics\n",
        "        metrics_map = {\n",
        "            'mean': 'mean',\n",
        "            'average': 'mean',\n",
        "            'median': 'median',\n",
        "            'min': 'min',\n",
        "            'minimum': 'min',\n",
        "            'max': 'max',\n",
        "            'maximum': 'max',\n",
        "            'std': 'std',\n",
        "            'standard deviation': 'std',\n",
        "            'sum': 'sum',\n",
        "            'count': 'count',\n",
        "            'unique': 'nunique',\n",
        "            'correlation': 'corr'\n",
        "        }\n",
        "\n",
        "        # Check for correlation request (special case)\n",
        "        if 'correlation' in input_string.lower() or 'corr' in input_string.lower():\n",
        "            if 'between' in input_string.lower() and 'and' in input_string.lower():\n",
        "                parts = input_string.lower().split('between')[1].split('and')\n",
        "                col1 = parts[0].strip()\n",
        "                col2 = parts[1].strip().split()[0].strip()\n",
        "\n",
        "                # Check if columns exist\n",
        "                if col1 in df.columns and col2 in df.columns:\n",
        "                    corr_value = df[col1].corr(df[col2])\n",
        "                    return f\"The correlation between {col1} and {col2} is {corr_value:.4f}\"\n",
        "                else:\n",
        "                    return f\"Columns not found. Available columns are: {', '.join(df.columns)}\"\n",
        "            else:\n",
        "                # Return full correlation matrix\n",
        "                corr_matrix = df.select_dtypes(include=['number']).corr()\n",
        "                return f\"Correlation Matrix:\\n{corr_matrix.round(2).to_string()}\"\n",
        "\n",
        "        # For other metrics\n",
        "        requested_metrics = []\n",
        "        for metric_name, func_name in metrics_map.items():\n",
        "            if metric_name in input_string.lower():\n",
        "                requested_metrics.append(func_name)\n",
        "\n",
        "        # If no specific metrics mentioned, calculate basic stats\n",
        "        if not requested_metrics:\n",
        "            requested_metrics = ['mean', 'median', 'min', 'max', 'std']\n",
        "\n",
        "        # Find requested columns\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # If no specific columns mentioned, use all numeric columns\n",
        "        if not columns:\n",
        "            columns = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "        # Calculate metrics\n",
        "        results = {}\n",
        "        for col in columns:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            col_results = {}\n",
        "            for metric in requested_metrics:\n",
        "                if metric == 'mean':\n",
        "                    col_results['mean'] = df[col].mean()\n",
        "                elif metric == 'median':\n",
        "                    col_results['median'] = df[col].median()\n",
        "                elif metric == 'min':\n",
        "                    col_results['min'] = df[col].min()\n",
        "                elif metric == 'max':\n",
        "                    col_results['max'] = df[col].max()\n",
        "                elif metric == 'std':\n",
        "                    col_results['std'] = df[col].std()\n",
        "                elif metric == 'sum':\n",
        "                    col_results['sum'] = df[col].sum()\n",
        "                elif metric == 'count':\n",
        "                    col_results['count'] = df[col].count()\n",
        "                elif metric == 'nunique':\n",
        "                    col_results['unique_values'] = df[col].nunique()\n",
        "\n",
        "            results[col] = col_results\n",
        "\n",
        "        # Format results\n",
        "        output = \"Data Metrics Analysis:\\n\"\n",
        "        for col, metrics in results.items():\n",
        "            output += f\"\\n{col}:\\n\"\n",
        "            for metric_name, value in metrics.items():\n",
        "                # Use format specifier only if value is a float\n",
        "                if isinstance(value, float):\n",
        "                    output += f\"  - {metric_name}: {value:.4f}\\n\"\n",
        "                else:\n",
        "                    output += f\"  - {metric_name}: {value}\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating metrics: {str(e)}. Please check column names and metrics requested.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# 2. Chart Generation Tool\n",
        "def generate_chart(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate charts based on columns and chart type and save to local filesystem.\n",
        "    Input should specify columns to visualize and optionally chart type.\n",
        "    Example: 'Create a chart of sales vs time' or 'Plot revenue against month as bar chart'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify columns mentioned\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # Identify chart type\n",
        "        chart_type = None\n",
        "        if 'bar' in input_string.lower() or 'histogram' in input_string.lower():\n",
        "            chart_type = 'bar'\n",
        "        elif 'scatter' in input_string.lower():\n",
        "            chart_type = 'scatter'\n",
        "        elif 'line' in input_string.lower():\n",
        "            chart_type = 'line'\n",
        "        elif 'pie' in input_string.lower():\n",
        "            chart_type = 'pie'\n",
        "        elif 'box' in input_string.lower() or 'boxplot' in input_string.lower():\n",
        "            chart_type = 'box'\n",
        "        elif 'heat' in input_string.lower() or 'heatmap' in input_string.lower():\n",
        "            chart_type = 'heatmap'\n",
        "\n",
        "        # If no columns specified or found\n",
        "        if not columns:\n",
        "            return f\"Please specify which columns to chart. Available columns: {', '.join(df.columns)}\"\n",
        "\n",
        "        # Determine best chart type if not specified\n",
        "        if not chart_type:\n",
        "            if len(columns) == 1:\n",
        "                # For a single column, use histogram for numeric, bar chart for categorical\n",
        "                if df[columns[0]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'histogram'\n",
        "                else:\n",
        "                    chart_type = 'bar'\n",
        "            elif len(columns) == 2:\n",
        "                # For two columns, if both numeric use scatter plot\n",
        "                if df[columns[0]].dtype in ['int64', 'float64'] and df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'scatter'\n",
        "                # If one is categorical and other numeric, use bar or line\n",
        "                elif df[columns[0]].dtype in ['int64', 'float64'] or df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    if 'time' in columns[0].lower() or 'date' in columns[0].lower() or 'month' in columns[0].lower() or 'year' in columns[0].lower():\n",
        "                        chart_type = 'line'\n",
        "                    else:\n",
        "                        chart_type = 'bar'\n",
        "                else:\n",
        "                    chart_type = 'heatmap'\n",
        "            else:\n",
        "                # For more than two columns, default to correlation heatmap for numeric columns\n",
        "                chart_type = 'heatmap'\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        if chart_type == 'bar':\n",
        "            if len(columns) == 1:\n",
        "                sns.countplot(x=columns[0], data=df)\n",
        "                plt.title(f\"Count of {columns[0]}\")\n",
        "            else:\n",
        "                # Assuming first column is x-axis, second is y-axis\n",
        "                sns.barplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"{columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'histogram':\n",
        "            sns.histplot(df[columns[0]], kde=True)\n",
        "            plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'scatter':\n",
        "            sns.scatterplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} vs {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'line':\n",
        "            sns.lineplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} over {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'pie':\n",
        "            # For pie charts, limit to top categories if categorical\n",
        "            if df[columns[0]].nunique() > 10:\n",
        "                top_values = df[columns[0]].value_counts().nlargest(10)\n",
        "                plt.pie(top_values, labels=top_values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Top 10 Categories in {columns[0]}\")\n",
        "            else:\n",
        "                values = df[columns[0]].value_counts()\n",
        "                plt.pie(values, labels=values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'box':\n",
        "            if len(columns) == 1:\n",
        "                sns.boxplot(y=columns[0], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[0]}\")\n",
        "            else:\n",
        "                sns.boxplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'heatmap':\n",
        "            if len(columns) == 2:\n",
        "                # Create a crosstab for categorical columns\n",
        "                cross_tab = pd.crosstab(df[columns[0]], df[columns[1]])\n",
        "                sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\", fmt='d')\n",
        "                plt.title(f\"Heatmap of {columns[0]} vs {columns[1]}\")\n",
        "            else:\n",
        "                # Correlation heatmap for multiple numeric columns\n",
        "                corr_columns = [col for col in columns if df[col].dtype in ['int64', 'float64']]\n",
        "                if not corr_columns:\n",
        "                    return \"Cannot create correlation heatmap - no numeric columns specified.\"\n",
        "                corr_matrix = df[corr_columns].corr()\n",
        "                sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "                plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Generate a descriptive filename based on chart type and columns\n",
        "        import os\n",
        "        import datetime\n",
        "\n",
        "        # Create charts directory if it doesn't exist\n",
        "        charts_dir = \"data_charts\"\n",
        "        if not os.path.exists(charts_dir):\n",
        "            os.makedirs(charts_dir)\n",
        "\n",
        "        # Create filename with timestamp and chart details\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        columns_str = \"_\".join([col.replace(\" \", \"_\") for col in columns])\n",
        "        filename = f\"{charts_dir}/{chart_type}_{columns_str}_{timestamp}.png\"\n",
        "\n",
        "        # Save the chart to the file\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Return information about the saved file\n",
        "        chart_info = f\"Generated a {chart_type} chart using columns: {', '.join(columns)}. \\nChart saved to: {os.path.abspath(filename)}\"\n",
        "\n",
        "        return chart_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating chart: {str(e)}. Please check your column names and chart request.\"\n",
        "\n",
        "# 3. Trend Analysis Tool\n",
        "def analyze_trends(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze trends in the data based on specified columns.\n",
        "    Input should specify which columns to analyze for trends.\n",
        "    Example: 'Analyze trends in monthly sales revenue' or 'Find patterns in customer acquisition by region'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify time-related columns\n",
        "        time_columns = []\n",
        "        for col in df.columns:\n",
        "            if any(term in col.lower() for term in ['time', 'date', 'month', 'year', 'quarter', 'week', 'day']):\n",
        "                time_columns.append(col)\n",
        "\n",
        "        # Identify metric columns mentioned in the input\n",
        "        metric_columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower() and col not in time_columns:\n",
        "                metric_columns.append(col)\n",
        "\n",
        "        # If no specific metric columns mentioned, identify numeric columns that might be metrics\n",
        "        if not metric_columns:\n",
        "            potential_metrics = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            # Filter out any time-related columns from potential metrics\n",
        "            metric_columns = [col for col in potential_metrics if col not in time_columns]\n",
        "\n",
        "        # If no time column found, try to find one\n",
        "        if not time_columns:\n",
        "            # Look for columns that might represent sequences or order\n",
        "            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            for col in numeric_cols:\n",
        "                # Check if column has sequential values that might represent time\n",
        "                if df[col].nunique() > 5 and df[col].is_monotonic_increasing:\n",
        "                    time_columns.append(col)\n",
        "                    break\n",
        "\n",
        "            # If still no time column, check if index might represent time\n",
        "            if not time_columns and df.index.is_monotonic_increasing:\n",
        "                df['index_as_sequence'] = df.index\n",
        "                time_columns.append('index_as_sequence')\n",
        "\n",
        "        # If we have both time and metric columns, analyze trends\n",
        "        results = []\n",
        "        if time_columns and metric_columns:\n",
        "            primary_time_col = time_columns[0]\n",
        "\n",
        "            for metric in metric_columns:\n",
        "                if df[metric].dtype not in ['int64', 'float64']:\n",
        "                    continue\n",
        "\n",
        "                # Basic trend statistics\n",
        "                trend_data = df[[primary_time_col, metric]].dropna()\n",
        "\n",
        "                if len(trend_data) < 3:  # Need at least 3 points for meaningful trend\n",
        "                    results.append(f\"Insufficient data points for {metric} trend analysis\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate basic trend metrics\n",
        "                first_value = trend_data[metric].iloc[0]\n",
        "                last_value = trend_data[metric].iloc[-1]\n",
        "                overall_change = last_value - first_value\n",
        "                percent_change = (overall_change / first_value * 100) if first_value != 0 else float('inf')\n",
        "\n",
        "                # Calculate moving averages if enough data points\n",
        "                if len(trend_data) >= 5:\n",
        "                    trend_data['3pt_ma'] = trend_data[metric].rolling(window=3).mean()\n",
        "\n",
        "                    # Detect direction (increasing/decreasing/fluctuating)\n",
        "                    increases = 0\n",
        "                    decreases = 0\n",
        "                    for i in range(1, len(trend_data)):\n",
        "                        if trend_data[metric].iloc[i] > trend_data[metric].iloc[i-1]:\n",
        "                            increases += 1\n",
        "                        elif trend_data[metric].iloc[i] < trend_data[metric].iloc[i-1]:\n",
        "                            decreases += 1\n",
        "\n",
        "                    # Determine trend direction\n",
        "                    if increases > decreases * 2:\n",
        "                        direction = \"strongly increasing\"\n",
        "                    elif increases > decreases:\n",
        "                        direction = \"moderately increasing\"\n",
        "                    elif decreases > increases * 2:\n",
        "                        direction = \"strongly decreasing\"\n",
        "                    elif decreases > increases:\n",
        "                        direction = \"moderately decreasing\"\n",
        "                    else:\n",
        "                        direction = \"fluctuating without clear direction\"\n",
        "\n",
        "                    # Check for seasonality (simplistic approach)\n",
        "                    if len(trend_data) >= 12:\n",
        "                        diffs = []\n",
        "                        for i in range(1, len(trend_data)):\n",
        "                            diffs.append(trend_data[metric].iloc[i] - trend_data[metric].iloc[i-1])\n",
        "                        sign_changes = sum(1 for i in range(1, len(diffs)) if (diffs[i] > 0 and diffs[i-1] < 0) or (diffs[i] < 0 and diffs[i-1] > 0))\n",
        "                        if sign_changes >= len(diffs) // 3:\n",
        "                            seasonality = \"data shows possible cyclical patterns\"\n",
        "                        else:\n",
        "                            seasonality = \"no clear seasonal patterns detected\"\n",
        "                    else:\n",
        "                        seasonality = \"insufficient data to detect seasonality\"\n",
        "\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Direction: {direction}\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Volatility: {trend_data[metric].std():.2f}\\n\"\n",
        "                    result += f\"- {seasonality}\\n\"\n",
        "                else:\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Limited data points available for detailed trend analysis\\n\"\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "            # Combine results\n",
        "            final_output = \"\\n\".join(results)\n",
        "            return final_output\n",
        "        else:\n",
        "            if not time_columns:\n",
        "                return \"Could not identify any time-related columns for trend analysis. Please specify which column represents time or sequence.\"\n",
        "            if not metric_columns:\n",
        "                return f\"Please specify which metrics to analyze for trends. Available numeric columns: {', '.join(df.select_dtypes(include=['number']).columns)}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing trends: {str(e)}. Please check your column names and trend request.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# 4. Data Summary Tool\n",
        "def summarize_data_contents(input_string: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Summarize the contents of the dataframe, including column names, data types,\n",
        "    and sample values for each column to help understand the data structure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Get basic dataframe info\n",
        "        num_rows, num_cols = df.shape\n",
        "        column_info = []\n",
        "\n",
        "        # Build summary for each column\n",
        "        for column in df.columns:\n",
        "            col_type = str(df[column].dtype)\n",
        "            unique_values = df[column].nunique()\n",
        "\n",
        "            # Get sample values based on data type and uniqueness\n",
        "            if unique_values <= 10:\n",
        "                # For columns with few unique values, show all of them with counts\n",
        "                value_counts = df[column].value_counts().head(10).to_dict()\n",
        "                sample_values = \", \".join([f\"{k} ({v})\" for k, v in value_counts.items()])\n",
        "            elif df[column].dtype == 'object':\n",
        "                # For text columns, show a few samples\n",
        "                sample_values = \", \".join([f'\"{x}\"' for x in df[column].dropna().sample(min(5, len(df))).tolist()])\n",
        "            else:\n",
        "                # For numeric columns, show range and a few examples\n",
        "                sample_values = f\"Range: {df[column].min()} to {df[column].max()}, \"\n",
        "                sample_values += f\"Examples: {', '.join(map(str, df[column].dropna().sample(min(3, len(df))).tolist()))}\"\n",
        "\n",
        "            # Check for missing values\n",
        "            missing = df[column].isna().sum()\n",
        "            missing_pct = missing / len(df) * 100\n",
        "\n",
        "            column_info.append({\n",
        "                \"column\": column,\n",
        "                \"type\": col_type,\n",
        "                \"unique_values\": unique_values,\n",
        "                \"missing\": f\"{missing} ({missing_pct:.1f}%)\",\n",
        "                \"sample_values\": sample_values\n",
        "            })\n",
        "\n",
        "        # Format the output\n",
        "        result = f\"DataFrame Summary: {num_rows} rows Ã— {num_cols} columns\\n\\n\"\n",
        "\n",
        "        for info in column_info:\n",
        "            result += f\"Column: {info['column']}\\n\"\n",
        "            result += f\"  - Type: {info['type']}\\n\"\n",
        "            result += f\"  - Unique Values: {info['unique_values']}\\n\"\n",
        "            result += f\"  - Missing Values: {info['missing']}\\n\"\n",
        "            result += f\"  - Sample Values: {info['sample_values']}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error summarizing data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# 5. Data Availability Tool\n",
        "def check_data_availability(input_string: str = \"\") -> str:\n",
        "    try:\n",
        "        if hasattr(builtins, 'df') and builtins.df is not None:\n",
        "            columns = list(builtins.df.columns)\n",
        "            return f\"Data is available with {len(builtins.df)} rows and {len(columns)} columns: {', '.join(columns)}\"\n",
        "        else:\n",
        "            return \"No data has been loaded yet. Please upload a CSV file first.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error checking data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# Create Tools\n",
        "metric_calculator = Tool(\n",
        "    name=\"calculate_metrics\",\n",
        "    func=calculate_metrics,\n",
        "    description=\"Calculate statistical metrics from the dataframe. Use when asked about averages, totals, statistics, correlations, or any numerical analysis of the data.\"\n",
        ")\n",
        "\n",
        "chart_generator = Tool(\n",
        "    name=\"generate_chart\",\n",
        "    func=generate_chart,\n",
        "    description=\"Generate charts and visualizations from the dataframe. Use when asked to plot, chart, visualize, or graph any data columns.\"\n",
        ")\n",
        "\n",
        "trend_analyzer = Tool(\n",
        "    name=\"analyze_trends\",\n",
        "    func=analyze_trends,\n",
        "    description=\"Analyze trends, patterns, and changes over time in the data. Use when asked about growth, decline, seasonality, or patterns in the data.\"\n",
        ")\n",
        "\n",
        "data_summarizer = Tool(\n",
        "    name=\"summarize_data\",\n",
        "    func=summarize_data_contents,\n",
        "    description=\"Analyze and summarize the contents of each column in the dataframe, including data types, unique values, and sample values. Use when asked about the structure of the data, what values are in columns, or to understand the dataset contents.\"\n",
        ")\n",
        "\n",
        "\n",
        "data_checker = Tool(\n",
        "    name=\"check_data\",\n",
        "    func=check_data_availability,\n",
        "    description=\"Check if data is loaded and show basic information about the available dataframe.\"\n",
        ")\n",
        "\n",
        "# Create the conversational prompt template\n",
        "conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a knowledgeable and insightful business analyst assistant.\n",
        "    Your goal is to help users analyze and understand their business data through clear explanations and visualizations.\n",
        "\n",
        "    When interacting with the user:\n",
        "    1. Help them understand what insights they can extract from their data\n",
        "    2. Explain business metrics and trends in plain language\n",
        "    3. Suggest appropriate visualizations based on the data type. If you do create a visualization, tell them where to find it.\n",
        "    4. Provide context for statistical findings\n",
        "    5. Suggest follow-up analyses when appropriate\n",
        "\n",
        "    Always verify that data is available before attempting analysis.\n",
        "    If the user hasn't uploaded data yet, kindly remind them to upload a CSV file first.\n",
        "\n",
        "    When using the metric calculator tool:\n",
        "    - Ask clarifying questions about which metrics they're interested in\n",
        "    - Explain what each metric means in business terms\n",
        "\n",
        "    When using the chart generator tool:\n",
        "    - Recommend appropriate chart types based on the data\n",
        "    - Explain what the visualization reveals about the data\n",
        "\n",
        "    When using the trend analyzer tool:\n",
        "    - Focus on identifying patterns and their business implications\n",
        "    - Connect trends to potential business actions\n",
        "\n",
        "    Always maintain a conversational, helpful tone and focus on one question at a time to keep the dialogue flowing naturally.\"\"\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Initialize the model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "\n",
        "\n",
        "# Create the conversational chain\n",
        "conversation_chain = conversation_prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# Custom process function to handle agent output with embedded images\n",
        "def process_agent_output(output):\n",
        "    # Check if output contains base64 image data\n",
        "    if \"data:image/png;base64,\" in output:\n",
        "        # Extract info text (part before the base64 data)\n",
        "        info_text = output.split(\"data:image/png;base64,\")[0].strip()\n",
        "\n",
        "        # Extract base64 data\n",
        "        base64_pattern = r\"data:image/png;base64,([^\\\"'\\s]+)\"\n",
        "        match = re.search(base64_pattern, output)\n",
        "\n",
        "        if match:\n",
        "            base64_data = match.group(1)\n",
        "            try:\n",
        "                # Decode base64 and display image\n",
        "                image_data = base64.b64decode(base64_data)\n",
        "                display(Image(data=image_data))\n",
        "\n",
        "                # Return only the text part of the response\n",
        "                return info_text\n",
        "            except Exception as e:\n",
        "                return f\"{info_text}\\n[Error displaying visualization: {str(e)}]\"\n",
        "        else:\n",
        "            return output\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "# Modified initialize_agent call with a custom callback to process the output\n",
        "class AgentOutputHandler:\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def run(self, input_text):\n",
        "        response = self.agent.run(input_text)\n",
        "        return process_agent_output(response)\n",
        "\n",
        "# Initialize agent with tools\n",
        "conversation_agent = initialize_agent(\n",
        "    tools=[data_checker, metric_calculator, chart_generator, trend_analyzer, data_summarizer],\n",
        "    llm=model,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True),\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "\n",
        "import textwrap\n",
        "from colorama import Fore, Style\n",
        "\n",
        "# Function to start the business analysis session\n",
        "def start_gpt_session():\n",
        "    print(f\"{Fore.GREEN}Business Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?{Style.RESET_ALL}\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")  # Regular input prompt\n",
        "        print(f\"{Fore.BLUE}You: {user_input}{Style.RESET_ALL}\")  # Display user input in blue\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye', \"fuck you\", \"fuck off\"]:\n",
        "            print(f\"{Fore.GREEN}Business Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.{Style.RESET_ALL}\")\n",
        "            break\n",
        "\n",
        "        # Direct agent response - no more processing needed\n",
        "        response = conversation_agent.run(user_input)\n",
        "\n",
        "        # Wrap the response text to a fixed width for better readability\n",
        "        wrapped_response = textwrap.fill(response, width=80)  # Adjust width as needed\n",
        "        print(f\"{Fore.GREEN}Business Analyst Assistant:\\n{wrapped_response}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "b33kIXEl1ht2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee28b75-83eb-4d90-e11c-f3c0ac7da22e"
      },
      "id": "b33kIXEl1ht2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c5033719e356>:560: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini 2.0 Flash Lite Agent"
      ],
      "metadata": {
        "id": "_XW4qFi51ZGk"
      },
      "id": "_XW4qFi51ZGk"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from typing import Optional\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from IPython.display import Image, display\n",
        "import base64\n",
        "import re\n",
        "import builtins\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# 1. Data Metrics Calculator Tool\n",
        "def calculate_metrics(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculate statistical metrics from the dataframe based on specified columns.\n",
        "    Input should be a string containing column names and desired metrics.\n",
        "    Example: 'Calculate mean, median, and standard deviation for sales_revenue'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Parse input to identify columns and metrics\n",
        "        metrics_map = {\n",
        "            'mean': 'mean',\n",
        "            'average': 'mean',\n",
        "            'median': 'median',\n",
        "            'min': 'min',\n",
        "            'minimum': 'min',\n",
        "            'max': 'max',\n",
        "            'maximum': 'max',\n",
        "            'std': 'std',\n",
        "            'standard deviation': 'std',\n",
        "            'sum': 'sum',\n",
        "            'count': 'count',\n",
        "            'unique': 'nunique',\n",
        "            'correlation': 'corr'\n",
        "        }\n",
        "\n",
        "        # Check for correlation request (special case)\n",
        "        if 'correlation' in input_string.lower() or 'corr' in input_string.lower():\n",
        "            if 'between' in input_string.lower() and 'and' in input_string.lower():\n",
        "                parts = input_string.lower().split('between')[1].split('and')\n",
        "                col1 = parts[0].strip()\n",
        "                col2 = parts[1].strip().split()[0].strip()\n",
        "\n",
        "                # Check if columns exist\n",
        "                if col1 in df.columns and col2 in df.columns:\n",
        "                    corr_value = df[col1].corr(df[col2])\n",
        "                    return f\"The correlation between {col1} and {col2} is {corr_value:.4f}\"\n",
        "                else:\n",
        "                    return f\"Columns not found. Available columns are: {', '.join(df.columns)}\"\n",
        "            else:\n",
        "                # Return full correlation matrix\n",
        "                corr_matrix = df.select_dtypes(include=['number']).corr()\n",
        "                return f\"Correlation Matrix:\\n{corr_matrix.round(2).to_string()}\"\n",
        "\n",
        "        # For other metrics\n",
        "        requested_metrics = []\n",
        "        for metric_name, func_name in metrics_map.items():\n",
        "            if metric_name in input_string.lower():\n",
        "                requested_metrics.append(func_name)\n",
        "\n",
        "        # If no specific metrics mentioned, calculate basic stats\n",
        "        if not requested_metrics:\n",
        "            requested_metrics = ['mean', 'median', 'min', 'max', 'std']\n",
        "\n",
        "        # Find requested columns\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # If no specific columns mentioned, use all numeric columns\n",
        "        if not columns:\n",
        "            columns = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "        # Calculate metrics\n",
        "        results = {}\n",
        "        for col in columns:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            col_results = {}\n",
        "            for metric in requested_metrics:\n",
        "                if metric == 'mean':\n",
        "                    col_results['mean'] = df[col].mean()\n",
        "                elif metric == 'median':\n",
        "                    col_results['median'] = df[col].median()\n",
        "                elif metric == 'min':\n",
        "                    col_results['min'] = df[col].min()\n",
        "                elif metric == 'max':\n",
        "                    col_results['max'] = df[col].max()\n",
        "                elif metric == 'std':\n",
        "                    col_results['std'] = df[col].std()\n",
        "                elif metric == 'sum':\n",
        "                    col_results['sum'] = df[col].sum()\n",
        "                elif metric == 'count':\n",
        "                    col_results['count'] = df[col].count()\n",
        "                elif metric == 'nunique':\n",
        "                    col_results['unique_values'] = df[col].nunique()\n",
        "\n",
        "            results[col] = col_results\n",
        "\n",
        "        # Format results\n",
        "        output = \"Data Metrics Analysis:\\n\"\n",
        "        for col, metrics in results.items():\n",
        "            output += f\"\\n{col}:\\n\"\n",
        "            for metric_name, value in metrics.items():\n",
        "                # Use format specifier only if value is a float\n",
        "                if isinstance(value, float):\n",
        "                    output += f\"  - {metric_name}: {value:.4f}\\n\"\n",
        "                else:\n",
        "                    output += f\"  - {metric_name}: {value}\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating metrics: {str(e)}. Please check column names and metrics requested.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# 2. Chart Generation Tool\n",
        "def generate_chart(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate charts based on columns and chart type and save to local filesystem.\n",
        "    Input should specify columns to visualize and optionally chart type.\n",
        "    Example: 'Create a chart of sales vs time' or 'Plot revenue against month as bar chart'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify columns mentioned\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower():\n",
        "                columns.append(col)\n",
        "\n",
        "        # Identify chart type\n",
        "        chart_type = None\n",
        "        if 'bar' in input_string.lower() or 'histogram' in input_string.lower():\n",
        "            chart_type = 'bar'\n",
        "        elif 'scatter' in input_string.lower():\n",
        "            chart_type = 'scatter'\n",
        "        elif 'line' in input_string.lower():\n",
        "            chart_type = 'line'\n",
        "        elif 'pie' in input_string.lower():\n",
        "            chart_type = 'pie'\n",
        "        elif 'box' in input_string.lower() or 'boxplot' in input_string.lower():\n",
        "            chart_type = 'box'\n",
        "        elif 'heat' in input_string.lower() or 'heatmap' in input_string.lower():\n",
        "            chart_type = 'heatmap'\n",
        "\n",
        "        # If no columns specified or found\n",
        "        if not columns:\n",
        "            return f\"Please specify which columns to chart. Available columns: {', '.join(df.columns)}\"\n",
        "\n",
        "        # Determine best chart type if not specified\n",
        "        if not chart_type:\n",
        "            if len(columns) == 1:\n",
        "                # For a single column, use histogram for numeric, bar chart for categorical\n",
        "                if df[columns[0]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'histogram'\n",
        "                else:\n",
        "                    chart_type = 'bar'\n",
        "            elif len(columns) == 2:\n",
        "                # For two columns, if both numeric use scatter plot\n",
        "                if df[columns[0]].dtype in ['int64', 'float64'] and df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    chart_type = 'scatter'\n",
        "                # If one is categorical and other numeric, use bar or line\n",
        "                elif df[columns[0]].dtype in ['int64', 'float64'] or df[columns[1]].dtype in ['int64', 'float64']:\n",
        "                    if 'time' in columns[0].lower() or 'date' in columns[0].lower() or 'month' in columns[0].lower() or 'year' in columns[0].lower():\n",
        "                        chart_type = 'line'\n",
        "                    else:\n",
        "                        chart_type = 'bar'\n",
        "                else:\n",
        "                    chart_type = 'heatmap'\n",
        "            else:\n",
        "                # For more than two columns, default to correlation heatmap for numeric columns\n",
        "                chart_type = 'heatmap'\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        if chart_type == 'bar':\n",
        "            if len(columns) == 1:\n",
        "                sns.countplot(x=columns[0], data=df)\n",
        "                plt.title(f\"Count of {columns[0]}\")\n",
        "            else:\n",
        "                # Assuming first column is x-axis, second is y-axis\n",
        "                sns.barplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"{columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'histogram':\n",
        "            sns.histplot(df[columns[0]], kde=True)\n",
        "            plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'scatter':\n",
        "            sns.scatterplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} vs {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'line':\n",
        "            sns.lineplot(x=columns[0], y=columns[1], data=df)\n",
        "            plt.title(f\"{columns[1]} over {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'pie':\n",
        "            # For pie charts, limit to top categories if categorical\n",
        "            if df[columns[0]].nunique() > 10:\n",
        "                top_values = df[columns[0]].value_counts().nlargest(10)\n",
        "                plt.pie(top_values, labels=top_values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Top 10 Categories in {columns[0]}\")\n",
        "            else:\n",
        "                values = df[columns[0]].value_counts()\n",
        "                plt.pie(values, labels=values.index, autopct='%1.1f%%')\n",
        "                plt.title(f\"Distribution of {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'box':\n",
        "            if len(columns) == 1:\n",
        "                sns.boxplot(y=columns[0], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[0]}\")\n",
        "            else:\n",
        "                sns.boxplot(x=columns[0], y=columns[1], data=df)\n",
        "                plt.title(f\"Box Plot of {columns[1]} by {columns[0]}\")\n",
        "\n",
        "        elif chart_type == 'heatmap':\n",
        "            if len(columns) == 2:\n",
        "                # Create a crosstab for categorical columns\n",
        "                cross_tab = pd.crosstab(df[columns[0]], df[columns[1]])\n",
        "                sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\", fmt='d')\n",
        "                plt.title(f\"Heatmap of {columns[0]} vs {columns[1]}\")\n",
        "            else:\n",
        "                # Correlation heatmap for multiple numeric columns\n",
        "                corr_columns = [col for col in columns if df[col].dtype in ['int64', 'float64']]\n",
        "                if not corr_columns:\n",
        "                    return \"Cannot create correlation heatmap - no numeric columns specified.\"\n",
        "                corr_matrix = df[corr_columns].corr()\n",
        "                sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "                plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Generate a descriptive filename based on chart type and columns\n",
        "        import os\n",
        "        import datetime\n",
        "\n",
        "        # Create charts directory if it doesn't exist\n",
        "        charts_dir = \"data_charts\"\n",
        "        if not os.path.exists(charts_dir):\n",
        "            os.makedirs(charts_dir)\n",
        "\n",
        "        # Create filename with timestamp and chart details\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        columns_str = \"_\".join([col.replace(\" \", \"_\") for col in columns])\n",
        "        filename = f\"{charts_dir}/{chart_type}_{columns_str}_{timestamp}.png\"\n",
        "\n",
        "        # Save the chart to the file\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Return information about the saved file\n",
        "        chart_info = f\"Generated a {chart_type} chart using columns: {', '.join(columns)}. \\nChart saved to: {os.path.abspath(filename)}\"\n",
        "\n",
        "        return chart_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating chart: {str(e)}. Please check your column names and chart request.\"\n",
        "\n",
        "# 3. Trend Analysis Tool\n",
        "def analyze_trends(input_string: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze trends in the data based on specified columns.\n",
        "    Input should specify which columns to analyze for trends.\n",
        "    Example: 'Analyze trends in monthly sales revenue' or 'Find patterns in customer acquisition by region'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Identify time-related columns\n",
        "        time_columns = []\n",
        "        for col in df.columns:\n",
        "            if any(term in col.lower() for term in ['time', 'date', 'month', 'year', 'quarter', 'week', 'day']):\n",
        "                time_columns.append(col)\n",
        "\n",
        "        # Identify metric columns mentioned in the input\n",
        "        metric_columns = []\n",
        "        for col in df.columns:\n",
        "            if col.lower() in input_string.lower() and col not in time_columns:\n",
        "                metric_columns.append(col)\n",
        "\n",
        "        # If no specific metric columns mentioned, identify numeric columns that might be metrics\n",
        "        if not metric_columns:\n",
        "            potential_metrics = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            # Filter out any time-related columns from potential metrics\n",
        "            metric_columns = [col for col in potential_metrics if col not in time_columns]\n",
        "\n",
        "        # If no time column found, try to find one\n",
        "        if not time_columns:\n",
        "            # Look for columns that might represent sequences or order\n",
        "            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "            for col in numeric_cols:\n",
        "                # Check if column has sequential values that might represent time\n",
        "                if df[col].nunique() > 5 and df[col].is_monotonic_increasing:\n",
        "                    time_columns.append(col)\n",
        "                    break\n",
        "\n",
        "            # If still no time column, check if index might represent time\n",
        "            if not time_columns and df.index.is_monotonic_increasing:\n",
        "                df['index_as_sequence'] = df.index\n",
        "                time_columns.append('index_as_sequence')\n",
        "\n",
        "        # If we have both time and metric columns, analyze trends\n",
        "        results = []\n",
        "        if time_columns and metric_columns:\n",
        "            primary_time_col = time_columns[0]\n",
        "\n",
        "            for metric in metric_columns:\n",
        "                if df[metric].dtype not in ['int64', 'float64']:\n",
        "                    continue\n",
        "\n",
        "                # Basic trend statistics\n",
        "                trend_data = df[[primary_time_col, metric]].dropna()\n",
        "\n",
        "                if len(trend_data) < 3:  # Need at least 3 points for meaningful trend\n",
        "                    results.append(f\"Insufficient data points for {metric} trend analysis\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate basic trend metrics\n",
        "                first_value = trend_data[metric].iloc[0]\n",
        "                last_value = trend_data[metric].iloc[-1]\n",
        "                overall_change = last_value - first_value\n",
        "                percent_change = (overall_change / first_value * 100) if first_value != 0 else float('inf')\n",
        "\n",
        "                # Calculate moving averages if enough data points\n",
        "                if len(trend_data) >= 5:\n",
        "                    trend_data['3pt_ma'] = trend_data[metric].rolling(window=3).mean()\n",
        "\n",
        "                    # Detect direction (increasing/decreasing/fluctuating)\n",
        "                    increases = 0\n",
        "                    decreases = 0\n",
        "                    for i in range(1, len(trend_data)):\n",
        "                        if trend_data[metric].iloc[i] > trend_data[metric].iloc[i-1]:\n",
        "                            increases += 1\n",
        "                        elif trend_data[metric].iloc[i] < trend_data[metric].iloc[i-1]:\n",
        "                            decreases += 1\n",
        "\n",
        "                    # Determine trend direction\n",
        "                    if increases > decreases * 2:\n",
        "                        direction = \"strongly increasing\"\n",
        "                    elif increases > decreases:\n",
        "                        direction = \"moderately increasing\"\n",
        "                    elif decreases > increases * 2:\n",
        "                        direction = \"strongly decreasing\"\n",
        "                    elif decreases > increases:\n",
        "                        direction = \"moderately decreasing\"\n",
        "                    else:\n",
        "                        direction = \"fluctuating without clear direction\"\n",
        "\n",
        "                    # Check for seasonality (simplistic approach)\n",
        "                    if len(trend_data) >= 12:\n",
        "                        diffs = []\n",
        "                        for i in range(1, len(trend_data)):\n",
        "                            diffs.append(trend_data[metric].iloc[i] - trend_data[metric].iloc[i-1])\n",
        "                        sign_changes = sum(1 for i in range(1, len(diffs)) if (diffs[i] > 0 and diffs[i-1] < 0) or (diffs[i] < 0 and diffs[i-1] > 0))\n",
        "                        if sign_changes >= len(diffs) // 3:\n",
        "                            seasonality = \"data shows possible cyclical patterns\"\n",
        "                        else:\n",
        "                            seasonality = \"no clear seasonal patterns detected\"\n",
        "                    else:\n",
        "                        seasonality = \"insufficient data to detect seasonality\"\n",
        "\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Direction: {direction}\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Volatility: {trend_data[metric].std():.2f}\\n\"\n",
        "                    result += f\"- {seasonality}\\n\"\n",
        "                else:\n",
        "                    result = f\"Trend Analysis for {metric}:\\n\"\n",
        "                    result += f\"- Overall change: {overall_change:.2f} ({percent_change:.2f}%)\\n\"\n",
        "                    result += f\"- Limited data points available for detailed trend analysis\\n\"\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "            # Combine results\n",
        "            final_output = \"\\n\".join(results)\n",
        "            return final_output\n",
        "        else:\n",
        "            if not time_columns:\n",
        "                return \"Could not identify any time-related columns for trend analysis. Please specify which column represents time or sequence.\"\n",
        "            if not metric_columns:\n",
        "                return f\"Please specify which metrics to analyze for trends. Available numeric columns: {', '.join(df.select_dtypes(include=['number']).columns)}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing trends: {str(e)}. Please check your column names and trend request.\"\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# 4. Data Summary Tool\n",
        "def summarize_data_contents(input_string: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Summarize the contents of the dataframe, including column names, data types,\n",
        "    and sample values for each column to help understand the data structure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Access df from builtins to ensure we're getting the global variable\n",
        "        if not hasattr(builtins, 'df') or builtins.df is None:\n",
        "            return \"No data has been loaded. Please upload a CSV file first.\"\n",
        "\n",
        "        # Use the global df variable\n",
        "        df = builtins.df\n",
        "\n",
        "        # Get basic dataframe info\n",
        "        num_rows, num_cols = df.shape\n",
        "        column_info = []\n",
        "\n",
        "        # Build summary for each column\n",
        "        for column in df.columns:\n",
        "            col_type = str(df[column].dtype)\n",
        "            unique_values = df[column].nunique()\n",
        "\n",
        "            # Get sample values based on data type and uniqueness\n",
        "            if unique_values <= 10:\n",
        "                # For columns with few unique values, show all of them with counts\n",
        "                value_counts = df[column].value_counts().head(10).to_dict()\n",
        "                sample_values = \", \".join([f\"{k} ({v})\" for k, v in value_counts.items()])\n",
        "            elif df[column].dtype == 'object':\n",
        "                # For text columns, show a few samples\n",
        "                sample_values = \", \".join([f'\"{x}\"' for x in df[column].dropna().sample(min(5, len(df))).tolist()])\n",
        "            else:\n",
        "                # For numeric columns, show range and a few examples\n",
        "                sample_values = f\"Range: {df[column].min()} to {df[column].max()}, \"\n",
        "                sample_values += f\"Examples: {', '.join(map(str, df[column].dropna().sample(min(3, len(df))).tolist()))}\"\n",
        "\n",
        "            # Check for missing values\n",
        "            missing = df[column].isna().sum()\n",
        "            missing_pct = missing / len(df) * 100\n",
        "\n",
        "            column_info.append({\n",
        "                \"column\": column,\n",
        "                \"type\": col_type,\n",
        "                \"unique_values\": unique_values,\n",
        "                \"missing\": f\"{missing} ({missing_pct:.1f}%)\",\n",
        "                \"sample_values\": sample_values\n",
        "            })\n",
        "\n",
        "        # Format the output\n",
        "        result = f\"DataFrame Summary: {num_rows} rows Ã— {num_cols} columns\\n\\n\"\n",
        "\n",
        "        for info in column_info:\n",
        "            result += f\"Column: {info['column']}\\n\"\n",
        "            result += f\"  - Type: {info['type']}\\n\"\n",
        "            result += f\"  - Unique Values: {info['unique_values']}\\n\"\n",
        "            result += f\"  - Missing Values: {info['missing']}\\n\"\n",
        "            result += f\"  - Sample Values: {info['sample_values']}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error summarizing data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# 5. Data Availability Tool\n",
        "def check_data_availability(input_string: str = \"\") -> str:\n",
        "    try:\n",
        "        if hasattr(builtins, 'df') and builtins.df is not None:\n",
        "            columns = list(builtins.df.columns)\n",
        "            return f\"Data is available with {len(builtins.df)} rows and {len(columns)} columns: {', '.join(columns)}\"\n",
        "        else:\n",
        "            return \"No data has been loaded yet. Please upload a CSV file first.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error checking data: {str(e)}\"\n",
        "    pass\n",
        "\n",
        "# Create Tools\n",
        "metric_calculator = Tool(\n",
        "    name=\"calculate_metrics\",\n",
        "    func=calculate_metrics,\n",
        "    description=\"Calculate statistical metrics from the dataframe. Use when asked about averages, totals, statistics, correlations, or any numerical analysis of the data.\"\n",
        ")\n",
        "\n",
        "chart_generator = Tool(\n",
        "    name=\"generate_chart\",\n",
        "    func=generate_chart,\n",
        "    description=\"Generate charts and visualizations from the dataframe. Use when asked to plot, chart, visualize, or graph any data columns.\"\n",
        ")\n",
        "\n",
        "trend_analyzer = Tool(\n",
        "    name=\"analyze_trends\",\n",
        "    func=analyze_trends,\n",
        "    description=\"Analyze trends, patterns, and changes over time in the data. Use when asked about growth, decline, seasonality, or patterns in the data.\"\n",
        ")\n",
        "\n",
        "data_summarizer = Tool(\n",
        "    name=\"summarize_data\",\n",
        "    func=summarize_data_contents,\n",
        "    description=\"Analyze and summarize the contents of each column in the dataframe, including data types, unique values, and sample values. Use when asked about the structure of the data, what values are in columns, or to understand the dataset contents.\"\n",
        ")\n",
        "\n",
        "\n",
        "data_checker = Tool(\n",
        "    name=\"check_data\",\n",
        "    func=check_data_availability,\n",
        "    description=\"Check if data is loaded and show basic information about the available dataframe.\"\n",
        ")\n",
        "\n",
        "# Create the conversational prompt template\n",
        "conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a knowledgeable and insightful business analyst assistant.\n",
        "    Your goal is to help users analyze and understand their business data through clear explanations and visualizations.\n",
        "\n",
        "    When interacting with the user:\n",
        "    1. Help them understand what insights they can extract from their data\n",
        "    2. Explain business metrics and trends in plain language\n",
        "    3. Suggest appropriate visualizations based on the data type. If you do create a chart, tell them where to find it.\n",
        "    4. Provide context for statistical findings\n",
        "    5. Suggest follow-up analyses when appropriate\n",
        "\n",
        "    Always verify that data is available before attempting analysis.\n",
        "    If the user hasn't uploaded data yet, kindly remind them to upload a CSV file first.\n",
        "\n",
        "    When using the metric calculator tool:\n",
        "    - Ask clarifying questions about which metrics they're interested in\n",
        "    - Explain what each metric means in business terms\n",
        "\n",
        "    When using the chart generator tool:\n",
        "    - Recommend appropriate chart types based on the data\n",
        "    - Explain what the visualization reveals about the data\n",
        "\n",
        "    When using the trend analyzer tool:\n",
        "    - Focus on identifying patterns and their business implications\n",
        "    - Connect trends to potential business actions\n",
        "\n",
        "    Always maintain a conversational, helpful tone and focus on one question at a time to keep the dialogue flowing naturally.\"\"\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Initialize the model\n",
        "# 1. Ensure gemini_key is retrieved from environment variables\n",
        "gemini_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if not gemini_key:\n",
        "    raise ValueError(\"Gemini API key not found. Please set the 'GOOGLE_API_KEY' environment variable.\")\n",
        "\n",
        "genai.configure(api_key=gemini_key)\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    google_api_key=gemini_key,\n",
        "    temperature=0.1,\n",
        "    convert_system_message_to_human=True  # Gemini handles system messages differently\n",
        ")\n",
        "# Create the conversational chain\n",
        "conversation_chain = conversation_prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# Custom process function to handle agent output with embedded images\n",
        "def process_agent_output(output):\n",
        "    # Check if output contains base64 image data\n",
        "    if \"data:image/png;base64,\" in output:\n",
        "        # Extract info text (part before the base64 data)\n",
        "        info_text = output.split(\"data:image/png;base64,\")[0].strip()\n",
        "\n",
        "        # Extract base64 data\n",
        "        base64_pattern = r\"data:image/png;base64,([^\\\"'\\s]+)\"\n",
        "        match = re.search(base64_pattern, output)\n",
        "\n",
        "        if match:\n",
        "            base64_data = match.group(1)\n",
        "            try:\n",
        "                # Decode base64 and display image\n",
        "                image_data = base64.b64decode(base64_data)\n",
        "                display(Image(data=image_data))\n",
        "\n",
        "                # Return only the text part of the response\n",
        "                return info_text\n",
        "            except Exception as e:\n",
        "                return f\"{info_text}\\n[Error displaying visualization: {str(e)}]\"\n",
        "        else:\n",
        "            return output\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "# Modified initialize_agent call with a custom callback to process the output\n",
        "class AgentOutputHandler:\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def run(self, input_text):\n",
        "        response = self.agent.run(input_text)\n",
        "        return process_agent_output(response)\n",
        "\n",
        "# Initialize agent with tools\n",
        "conversation_agent = initialize_agent(\n",
        "    tools=[data_checker, metric_calculator, chart_generator, trend_analyzer, data_summarizer],\n",
        "    llm=model,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True),\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "import textwrap\n",
        "from colorama import Fore, Style\n",
        "\n",
        "# Function to start the business analysis session\n",
        "def start_gemini_session():\n",
        "    print(f\"{Fore.GREEN}Business Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?{Style.RESET_ALL}\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")  # Regular input prompt\n",
        "        print(f\"{Fore.BLUE}You: {user_input}{Style.RESET_ALL}\")  # Display user input in blue\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye', \"fuck you\", \"fuck off\"]:\n",
        "            print(f\"{Fore.GREEN}Business Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.{Style.RESET_ALL}\")\n",
        "            break\n",
        "\n",
        "        # Direct agent response - no more processing needed\n",
        "        response = conversation_agent.run(user_input)\n",
        "\n",
        "        # Wrap the response text to a fixed width for better readability\n",
        "        wrapped_response = textwrap.fill(response, width=80)  # Adjust width as needed\n",
        "        print(f\"{Fore.GREEN}Business Analyst Assistant:\\n{wrapped_response}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "8jNMZyin1Yfh"
      },
      "id": "8jNMZyin1Yfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Charts are saved in the data_charts folder in the left of your Jupyter Notebook"
      ],
      "metadata": {
        "id": "jPmaDgN76oSn"
      },
      "id": "jPmaDgN76oSn"
    },
    {
      "cell_type": "code",
      "source": [
        "start_gpt_session()"
      ],
      "metadata": {
        "id": "PhtDGKyc1oab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb2991a-68ef-42ff-8529-9f17886d534a"
      },
      "id": "PhtDGKyc1oab",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?\u001b[0m\n",
            "You: summarize the data\n",
            "\u001b[34mYou: summarize the data\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "The data contains sales information from different retailers, regions, states,\n",
            "and cities. It includes details about the products sold, their prices, the\n",
            "number of units sold, total sales, operating profit, operating margin, and sales\n",
            "method. There are 9648 rows and 14 columns. The 'Unnamed: 0' column has all\n",
            "missing values. The other columns contain information about retailers, dates,\n",
            "regions, states, cities, products, prices, units sold, total sales, operating\n",
            "profit, operating margin, and sales method.\u001b[0m\n",
            "You: sales trend analysis\n",
            "\u001b[34mYou: sales trend analysis\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "The sales trend analysis reveals that the Retailer ID shows a moderately\n",
            "increasing trend, while Price per Unit, Units Sold, Total Sales, and Operating\n",
            "Profit all show moderately decreasing trends. The data suggests possible\n",
            "cyclical patterns for Price per Unit, Units Sold, and Total Sales.\u001b[0m\n",
            "You: quit\n",
            "\u001b[34mYou: quit\u001b[0m\n",
            "\u001b[32mBusiness Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_gemini_session()"
      ],
      "metadata": {
        "id": "EjN7M-KO105r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572c885c-d4c7-4cfc-c64f-e96b02cb7d1a"
      },
      "id": "EjN7M-KO105r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?\u001b[0m\n",
            "You: tell me about the data\n",
            "\u001b[34mYou: tell me about the data\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "The data contains information on sales from different retailers, regions,\n",
            "states, and cities. It includes details about the products sold, their prices,\n",
            "the number of units sold, total sales, operating profit, operating margin, and\n",
            "sales method. There are 9648 rows and 14 columns. The 'Unnamed: 0' column has\n",
            "all missing values. The other columns contain information about retailers,\n",
            "dates, regions, states, cities, products, prices, units sold, total sales,\n",
            "operating profit, operating margin, and sales method.\u001b[0m\n",
            "You: do a sales trend analysis\n",
            "\u001b[34mYou: do a sales trend analysis\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "The sales trend analysis reveals that the Retailer ID shows a moderately\n",
            "increasing trend, while Price per Unit, Units Sold, Total Sales, and Operating\n",
            "Profit all show moderately decreasing trends. The data suggests possible\n",
            "cyclical patterns for Price per Unit, Units Sold, and Total Sales.\u001b[0m\n",
            "You: generate a bar chart for product and total sales\n",
            "\u001b[34mYou: generate a bar chart for product and total sales\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "I have generated a bar chart showing the total sales for each product.\u001b[0m\n",
            "You: quit\n",
            "\u001b[34mYou: quit\u001b[0m\n",
            "\u001b[32mBusiness Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_llama_session()"
      ],
      "metadata": {
        "id": "3-3cmNh91zlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7140c57-f65b-4caa-d61e-99cd0c078be4"
      },
      "id": "3-3cmNh91zlZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant: Hello! I'm your business data analyst assistant. I can help you analyze your uploaded data, calculate metrics, create visualizations, and identify trends. What would you like to know about your data?\u001b[0m\n",
            "You: bar chart for average price per unit for each product category. add dollar signs to the y axis\n",
            "\u001b[34mYou: bar chart for average price per unit for each product category. add dollar signs to the y axis\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mBusiness Analyst Assistant:\n",
            "I have generated a bar chart showing the average price per unit for each product\n",
            "category, with dollar signs on the y-axis.\u001b[0m\n",
            "You: quit\n",
            "\u001b[34mYou: quit\u001b[0m\n",
            "\u001b[32mBusiness Analyst Assistant: Thanks for the analysis session! If you have more data to analyze in the future, I'll be here to help.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7J01v5vktvr"
      },
      "id": "a7J01v5vktvr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da110c8e688243e1ab6a64fa8ea388c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "ðŸ”‘ OpenAI Key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dd741e70b1cc43d39f52e3913f6ae972",
            "placeholder": "Enter your OpenAI API Key",
            "style": "IPY_MODEL_7dbcbd1767c24237ad3f22d611c17909",
            "value": "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"
          }
        },
        "dd741e70b1cc43d39f52e3913f6ae972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbcbd1767c24237ad3f22d611c17909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bb5774044e5471399a1efcded104709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "ðŸ¤— HF Key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1ec7c323c6b540eb99d8a8ebf73f1b80",
            "placeholder": "Enter your Hugging Face API Key",
            "style": "IPY_MODEL_449547ebe3ec48e1bc746450d8a34ee4",
            "value": "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"
          }
        },
        "1ec7c323c6b540eb99d8a8ebf73f1b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449547ebe3ec48e1bc746450d8a34ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac400e4cb9884ed69e236861246cfc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "âœ¨ Gemini Key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e4dfb0fd85d74187a58128502fdb178b",
            "placeholder": "Enter your Gemini API Key",
            "style": "IPY_MODEL_4fdf2ccf7f414527a1d89319118c94e6",
            "value": "AIzaSyAlPxPwOYqKpctP82SK8AA8imd2QhfqzHk"
          }
        },
        "e4dfb0fd85d74187a58128502fdb178b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fdf2ccf7f414527a1d89319118c94e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "327e0dd4465748498e7e93bf46e15055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "âœ… Set API Keys",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ff9b522561fd4abe855ccfe3ab595cad",
            "style": "IPY_MODEL_db75bb5ea3c44398b69e38107d1ceb8f",
            "tooltip": ""
          }
        },
        "ff9b522561fd4abe855ccfe3ab595cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db75bb5ea3c44398b69e38107d1ceb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "18ebc14e9a0a446ea2ddb01f61137c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_03c858a64d3d4f7cbcd29b75cbd9bfcd",
            "metadata": [
              {
                "name": "Adidas_Sales.csv",
                "type": "text/csv",
                "size": 1053460,
                "lastModified": 1740064122573
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_c7e4131c636f4cac8510e7d3bd5fa280"
          }
        },
        "03c858a64d3d4f7cbcd29b75cbd9bfcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e4131c636f4cac8510e7d3bd5fa280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "991cd48c490d48aca70f64e850e4ec60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "âœ… Process CSV",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_df867a363cb14fe08d0eba405a644f1c",
            "style": "IPY_MODEL_d1b574100af04142b9d68ec584434d5a",
            "tooltip": ""
          }
        },
        "df867a363cb14fe08d0eba405a644f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b574100af04142b9d68ec584434d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}